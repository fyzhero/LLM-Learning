# LLM-Learning
Paper learning for LLM



## 
Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful
https://arxiv.org/pdf/2507.07101
